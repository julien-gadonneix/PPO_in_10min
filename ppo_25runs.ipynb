{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"src\"))\n",
    "import torch\n",
    "from src.trainer import Trainer\n",
    "from src.models import NN_CartPole, NN_Humanoid, NN_Hopper\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif torch.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"You are using device: %s\" % device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment selection: only for the CartPole for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_env = \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "updates = {\"CartPole-v1\": 80, \"Humanoid-v5\": 1000, \"Hopper-v5\": 600} # CartPole-v1: [60, 80, 100]\n",
    "epochs = {\"CartPole-v1\": 20, \"Humanoid-v5\": 8, \"Hopper-v5\": 12} # CartPole-v1: [16, 20, 24] # Hopper-v5: [8, 12, 16] # Humanoid-v5: [6, 8, 10]\n",
    "N = {\"CartPole-v1\": 20, \"Humanoid-v5\": 48, \"Hopper-v5\": 4} # CartPole-v1: [16, 20, 24] # Hopper-v5: [2, 4, 6] # Humanoid-v5: [40, 48, 56]\n",
    "T = {\"CartPole-v1\": 128, \"Humanoid-v5\": 256, \"Hopper-v5\": 1024} # CartPole-v1: [64, 128, 256] # Hopper-v5: [512, 1024, 2048] # Humanoid-v5: [128, 256, 512]\n",
    "batches = {\"CartPole-v1\": 4, \"Humanoid-v5\": 4, \"Hopper-v5\": 16} # CartPole-v1: [2, 4, 8] # Hopper-v5: [8, 16, 32] # Humanoid-v5: [2, 4, 8]\n",
    "value_loss_coef = {\"CartPole-v1\": 0.5, \"Humanoid-v5\": 1.0, \"Hopper-v5\": 1.0} # CartPole-v1: [0.25, 0.5, 0.75] # Hopper-v5: [0.5, 1.0, 1.5] # Humanoid-v5: [0.5, 1.0, 1.5]\n",
    "entropy_bonus_coef = {\"CartPole-v1\": 1e-3, \"Humanoid-v5\": 1e-5, \"Hopper-v5\": 5e-5} # CartPole-v1: [5e-4, 1e-3, 5e-3] # Hopper-v5: [1e-5, 5e-5, 1e-4] # Humanoid-v5: [5e-5, 1e-5, 5e-4]\n",
    "clip_range = {\"CartPole-v1\": 0.1, \"Humanoid-v5\": 0.1, \"Hopper-v5\": 0.1} # CartPole-v1: [0.05, 0.1, 0.5] # Hopper-v5: [0.05, 0.1, 0.5] # Humanoid-v5: [0.05, 0.1, 0.5]\n",
    "learning_rate = {\"CartPole-v1\": 5e-4, \"Humanoid-v5\": 3e-4, \"Hopper-v5\": 1e-4} # CartPole-v1: [1e-4, 5e-4, 1e-3] # Hopper-v5: [1e-5, 1e-4, 1e-3] # Humanoid-v5: [1e-4, 3e-4, 5e-4]\n",
    "learning_rate_decay = {\"CartPole-v1\": 0.999, \"Humanoid-v5\": 0.99, \"Hopper-v5\": 0.999} # CartPole-v1: [0.99, 0.999, 0.9999] # Hopper-v5: [0.99, 0.999, 9999] # Humanoid-v5: [0.9, 0.99, 0.999]\n",
    "reward_scaling = {\"CartPole-v1\": 0.5, \"Humanoid-v5\": 0.01, \"Hopper-v5\": 0.1} # CartPole-v1: [0.1, 0.5, 1.0] # Hopper-v5: [0.05, 0.1, 0.5] # Humanoid-v5: [0.001, 0.01, 0.1]\n",
    "models = {\"CartPole-v1\": NN_CartPole, \"Humanoid-v5\": NN_Humanoid, \"Hopper-v5\": NN_Hopper}\n",
    "\n",
    "# Configurations\n",
    "configs = {\n",
    "    # Number of updates\n",
    "    'updates': updates[str_env],\n",
    "    # Number of epochs to train the model with sampled data.\n",
    "    'epochs': epochs[str_env],\n",
    "    # Number of worker processes\n",
    "    'N': N[str_env],\n",
    "    # Number of steps to run on each process for a single update\n",
    "    'T': T[str_env],\n",
    "    # Number of mini batches\n",
    "    'batches': batches[str_env],\n",
    "    # Value loss coefficient.\n",
    "    'value_loss_coef': value_loss_coef[str_env],\n",
    "    # Entropy bonus coefficient.\n",
    "    'entropy_bonus_coef': entropy_bonus_coef[str_env],\n",
    "    # Clip range.\n",
    "    'clip_range': clip_range[str_env],\n",
    "    # Learning rate.\n",
    "    'learning_rate': learning_rate[str_env],\n",
    "    # Learning rate decay.\n",
    "    'learning_rate_decay': learning_rate_decay[str_env],\n",
    "    # Model to use\n",
    "    'model': models[str_env](),\n",
    "    # Device to use for training\n",
    "    'device': device,\n",
    "    # Environment to use\n",
    "    'str_env': str_env,\n",
    "    # Reward scaling\n",
    "    'reward_scaling': reward_scaling[str_env]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25 runs to average the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_length = np.zeros((25*N[str_env], updates[str_env]))\n",
    "policy_loss_list = np.zeros((25, updates[str_env]*epochs[str_env]*batches[str_env]))\n",
    "value_loss_list = np.zeros((25, updates[str_env]*epochs[str_env]*batches[str_env]))\n",
    "entropy_bonus_list = np.zeros((25, updates[str_env]*epochs[str_env]*batches[str_env]))\n",
    "clip_fraction_list = np.zeros((25, updates[str_env]*epochs[str_env]*batches[str_env]))\n",
    "\n",
    "for i in range(25):\n",
    "    configs = {\n",
    "        # Number of updates\n",
    "        'updates': updates[str_env],\n",
    "        # Number of epochs to train the model with sampled data.\n",
    "        'epochs': epochs[str_env],\n",
    "        # Number of worker processes\n",
    "        'N': N[str_env],\n",
    "        # Number of steps to run on each process for a single update\n",
    "        'T': T[str_env],\n",
    "        # Number of mini batches\n",
    "        'batches': batches[str_env],\n",
    "        # Value loss coefficient.\n",
    "        'value_loss_coef': value_loss_coef[str_env],\n",
    "        # Entropy bonus coefficient.\n",
    "        'entropy_bonus_coef': entropy_bonus_coef[str_env],\n",
    "        # Clip range.\n",
    "        'clip_range': clip_range[str_env],\n",
    "        # Learning rate.\n",
    "        'learning_rate': learning_rate[str_env],\n",
    "        # Learning rate decay.\n",
    "        'learning_rate_decay': learning_rate_decay[str_env],\n",
    "        # Model to use\n",
    "        'model': models[str_env](),\n",
    "        # Device to use for training\n",
    "        'device': device,\n",
    "        # Environment to use\n",
    "        'str_env': str_env,\n",
    "        # Reward scaling\n",
    "        'reward_scaling': reward_scaling[str_env]\n",
    "    }\n",
    "    trainer = Trainer(**configs)\n",
    "    trainer.run_training_loop()\n",
    "    episodes_length[i*N[str_env]:(i+1)*N[str_env], :] = np.array(trainer.episode_length)\n",
    "    policy_loss_list[i, :] = -np.array(trainer.policy_loss_list)\n",
    "    value_loss_list[i, :] = np.array(trainer.value_loss_list)\n",
    "    entropy_bonus_list[i, :] = np.array(trainer.entropy_bonus_list)\n",
    "    clip_fraction_list[i, :] = np.array(trainer.clip_fraction_list)\n",
    "    trainer.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "mu = episodes_length.mean(axis=0)\n",
    "plt.plot(mu, label=\"Mean episode length across workers and runs\")\n",
    "sigma = episodes_length.std(axis=0)\n",
    "plt.axhline(y=T[str_env], color='r', linestyle='--', label=\"T\")\n",
    "plt.fill_between(np.arange(updates[str_env]), mu - sigma, np.minimum(T[str_env], mu + sigma), alpha=0.3, label=\"Standard deviation of the episode length across workers and runs\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Updates\")\n",
    "plt.ylabel(\"Episode Length\")\n",
    "plt.xlim(0, updates[str_env])\n",
    "plt.ylim(0, 1.1*T[str_env])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Policy Loss\", \"Value Loss\", \"Entropy Bonus\", \"Clip Fraction\"]\n",
    "vals = [policy_loss_list, value_loss_list, entropy_bonus_list, clip_fraction_list]\n",
    "for name, val in zip(names, vals):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    mu = val.mean(axis=0)\n",
    "    plt.plot(mu, label=\"Mean \" + name + \" across runs\")\n",
    "    sigma = val.std(axis=0)\n",
    "    plt.fill_between(np.arange(updates[str_env]*epochs[str_env]*batches[str_env]), mu-sigma, mu+sigma, alpha=0.3, label=\"Standard deviation of the \" + name + \" across runs\")\n",
    "    min_value = np.min(val)\n",
    "    max_value = np.max(val)\n",
    "    for i in range(updates[str_env]):\n",
    "        if i == updates[str_env]-1:\n",
    "            plt.vlines(i*epochs[str_env]*batches[str_env], min_value, max_value, color='k', linestyle='--', label=\"Updates\", alpha=0.5)\n",
    "        else:\n",
    "            plt.vlines(i*epochs[str_env]*batches[str_env], min_value, max_value, color='k', linestyle='--', alpha=0.5*(1-np.exp(-i/100)))\n",
    "    plt.xlabel(\"Updates x Epochs x Batches\")\n",
    "    plt.ylim(-0.01, 0.005)\n",
    "    plt.xlim(0, updates[str_env]*epochs[str_env]*batches[str_env])\n",
    "    plt.ylabel(name)\n",
    "    plt.grid()\n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_ic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
