# PPO_in_10min

## PPO code - All you need to know (80/20 rule applied)

### Bioengineering applications at stake !

[Acknowledgement: the code is inspired from [this code](https://github.com/labmlai/annotated_deep_learning_paper_implementations/tree/master/labml_nn/rl/ppo)]

My goal: Provide you with the keys to fully understand, explain, and implement a state-of-the-art RL method: Proximal Policy Optimization (PPO);

My tools: Python, PyTorch and Mathematical Theory;

Your takeaway: Enhanced understanding of RL techniques and recognised skills in AI, applied to PPO;

Bonus: A bioengineering application;

### Key Files

- **`src/`**: Contains all the application code.
- **`checkpoint/`**: Contains the pretrained models' weights.
- **`results/`**: Contains the figures and videos.

- **`ppo.ipynb`**: Run the PPO algoritmh in a step-by-step customization.
- **`ppo_noised.ipynb`**: Run the PPO algoritmh in a step-by-step customization.
- **`ppo_25runs.ipynb`**: Run the PPO algoritmh 25 times to average the plots.
